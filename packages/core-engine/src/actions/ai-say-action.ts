/**
 * AiSayAction - AIå‘ç”¨æˆ·ä¼ è¾¾ä¿¡æ¯ï¼ˆå¢å¼ºç‰ˆï¼‰
 *
 * ã€DDD è§†è§’ã€‘åº”ç”¨å±‚æœåŠ¡ - Action æ‰§è¡Œå™¨
 * è´Ÿè´£å°†è„šæœ¬ä¸­çš„ ai_say åŠ¨ä½œå®šä¹‰è½¬åŒ–ä¸ºå®é™…æ‰§è¡Œè¿‡ç¨‹
 *
 * æ ¸å¿ƒèƒ½åŠ›ï¼š
 * 1. å¤šè½®å¯¹è¯ï¼šåŸºäº max_rounds æ§åˆ¶ï¼Œæ”¯æŒä¸ç”¨æˆ·å¤šè½®äº¤äº’ç›´åˆ°ç†è§£
 * 2. æç¤ºè¯æ¨¡æ¿ï¼šä¸¤å±‚å˜é‡æ›¿æ¢ï¼ˆè„šæœ¬å˜é‡ + ç³»ç»Ÿå˜é‡ï¼‰
 * 3. ç†è§£åº¦è¯„ä¼°ï¼šLLM æ™ºèƒ½åˆ¤æ–­ç”¨æˆ·æ˜¯å¦å·²ç†è§£å†…å®¹
 * 4. æ™ºèƒ½é€€å‡ºï¼šåŸºäº exit_criteria è‡ªåŠ¨å†³ç­–æ˜¯å¦ç»“æŸå¯¹è¯
 * 5. å‘åå…¼å®¹ï¼šä¿ç•™ require_acknowledgment æœºåˆ¶
 *
 * ä¸šåŠ¡è§„åˆ™ï¼š
 * - æ¨¡æ¿æ¨¡å¼ï¼šå½“é…ç½® max_rounds æˆ– exit_criteria æ—¶å¯ç”¨
 * - å…¼å®¹æ¨¡å¼ï¼šç®€å•å•è½®å¯¹è¯ï¼Œä»…è¾“å‡ºé™æ€å†…å®¹
 * - é€€å‡ºå†³ç­–é¡ºåºï¼šmax_rounds > exit_criteria > llm_suggestion
 */

import { LLMOrchestrator } from '../engines/llm-orchestration/orchestrator.js';
import { PromptTemplateManager, TemplateResolver } from '../engines/prompt-template/index.js';

import { BaseAction } from './base-action.js';
import type { ActionContext, ActionResult } from './base-action.js';

/**
 * LLM è¾“å‡ºæ ¼å¼ï¼ˆæ”¯æŒæ–°æ—§ä¸¤ç§æ ¼å¼ï¼‰
 */
interface MainLineOutput {
  // æ–°æ ¼å¼å­—æ®µ
  content?: string;
  EXIT?: string;
  BRIEF?: string;
  safety_risk?: {
    detected: boolean;
    risk_type: string | null;
    confidence: 'high' | 'medium' | 'low';
    reason: string | null;
  };
  metadata?: {
    emotional_tone?: string;
    crisis_signal?: boolean;
    assessment?: {
      understanding_level: number;
      has_questions: boolean;
      expressed_understanding: boolean;
      reasoning: string;
    };
  };
  
  // æ—§æ ¼å¼å­—æ®µï¼ˆå‘åå…¼å®¹ï¼‰
  assessment?: {
    understanding_level: number;
    has_questions: boolean;
    expressed_understanding: boolean;
    reasoning: string;
  };
  response?: {
    [key: string]: string; // æ”¯æŒåŠ¨æ€è§’è‰²å
  };
  should_exit?: boolean;
  exit_reason?: string;
}

export class AiSayAction extends BaseAction {
  static actionType = 'ai_say';
  private llmOrchestrator?: LLMOrchestrator;
  private templateManager: PromptTemplateManager;
  private templateResolver: TemplateResolver;
  private useTemplateMode: boolean = false; // æ˜¯å¦ä½¿ç”¨æ¨¡æ¿æ¨¡å¼

  constructor(actionId: string, config: Record<string, any>, llmOrchestrator?: LLMOrchestrator) {
    super(actionId, config);
    this.llmOrchestrator = llmOrchestrator;

    const templateBasePath = this.resolveTemplatePath();
    console.log(`[AiSayAction] ğŸ“ Template path: ${templateBasePath}`);

    this.templateManager = new PromptTemplateManager(templateBasePath);
    // TemplateResolver éœ€è¦é¡¹ç›®æ ¹ç›®å½•ï¼Œä½†æ­¤æ—¶è¿˜æ²¡æœ‰contextï¼Œæš‚ä¸åˆå§‹åŒ–
    this.templateResolver = null as any; // å»¶è¿Ÿåˆå§‹åŒ–

    // maxRounds å’Œ exitCriteria å·²åœ¨ BaseAction ä¸­è®¾ç½®
    // åˆ¤æ–­æ˜¯å¦ä½¿ç”¨æ¨¡æ¿æ¨¡å¼ï¼šæœ‰ max_rounds æˆ– exit_criteria é…ç½®
    this.useTemplateMode =
      this.getConfig('max_rounds') !== undefined || this.getConfig('exit_criteria') !== undefined;

    // è®¾ç½®é€€å‡ºç­–ç•¥ï¼šai_say æ”¯æŒå¤šè½®é€€å‡º
    this.exitPolicy = {
      supportsExit: true,
      enabledSources: ['max_rounds', 'exit_criteria', 'llm_suggestion'],
    };
  }

  async execute(context: ActionContext, userInput?: string | null): Promise<ActionResult> {
    try {
      console.log(`[AiSayAction] ğŸ”µ Executing:`, {
        actionId: this.actionId,
        currentRound: this.currentRound,
        maxRounds: this.maxRounds,
        useTemplateMode: this.useTemplateMode,
      });

      // æ¨¡å¼é€‰æ‹©ï¼šæ¨¡æ¿æ¨¡å¼ vs å…¼å®¹æ¨¡å¼
      if (this.useTemplateMode && this.llmOrchestrator) {
        return await this.executeTemplateMode(context, userInput);
      } else {
        return await this.executeLegacyMode(context, userInput);
      }
    } catch (e: any) {
      console.error(`[AiSayAction] âŒ Execution error:`, e);
      return {
        success: false,
        completed: true,
        error: `ai_say execution error: ${e.message}`,
      };
    }
  }

  /**
   * æ¨¡æ¿æ¨¡å¼æ‰§è¡Œï¼ˆæ–°åŠŸèƒ½ï¼šå¤šè½®å¯¹è¯ + ç†è§£åº¦è¯„ä¼°ï¼‰
   */
  private async executeTemplateMode(
    context: ActionContext,
    _userInput?: string | null
  ): Promise<ActionResult> {
    // å¢åŠ è½®æ¬¡è®¡æ•°
    this.currentRound++;

    // è§„åˆ™1: æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è½®æ¬¡
    if (this.currentRound > this.maxRounds) {
      console.log(`[AiSayAction] âš ï¸ Reached max_rounds (${this.maxRounds}), force exit`);
      return {
        success: true,
        completed: true,
        aiMessage: null,
        metadata: {
          actionType: AiSayAction.actionType,
          exitDecision: {
            should_exit: true,
            reason: `è¾¾åˆ°æœ€å¤§è½®æ¬¡é™åˆ¶ (${this.maxRounds})`,
            decision_source: 'max_rounds',
          },
        },
      };
    }

    // 1. åŠ è½½æç¤ºè¯æ¨¡æ¿
    const { template, resolution } = await this.loadPromptTemplate(context);

    // 2. å‡†å¤‡å˜é‡
    const scriptVariables = this.extractScriptVariables(context);
    const systemVariables = this.buildSystemVariables(context);

    // 3. ä¸¤å±‚å˜é‡æ›¿æ¢
    const prompt = this.templateManager.substituteVariables(
      template.content,
      scriptVariables,
      systemVariables
    );

    console.log(`[AiSayAction] ğŸ“ Prompt prepared (${prompt.length} chars)`);

    // 4. è°ƒç”¨ LLM
    const llmResult = await this.llmOrchestrator!.generateText(prompt, {
      temperature: 0.7,
      maxTokens: 1000,
    });

    // 5. å®‰å…¨è¾¹ç•Œæ£€æµ‹
    const safetyCheck = this.checkSafetyBoundary(llmResult.text);
    if (!safetyCheck.passed) {
      console.warn(`[AiSayAction] âš ï¸ Safety boundary violations detected:`, safetyCheck.violations);
    }

    // 6. è§£æ LLM å“åº”ï¼ˆå¤„ç† markdown ä»£ç å—ï¼‰
    const jsonText = this.cleanJsonOutput(llmResult.text);

    let llmOutput: MainLineOutput;
    try {
      llmOutput = JSON.parse(jsonText);
    } catch (error: any) {
      console.error(`[AiSayAction] âŒ Failed to parse LLM output:`, llmResult.text);
      throw new Error(`Failed to parse LLM output: ${error.message}`);
    }

    // 7. é€€å‡ºå†³ç­–ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„ evaluateExitCondition æ–¹æ³•ï¼‰
    const exitDecision = this.evaluateExitCondition(context, llmOutput);

    console.log(`[AiSayAction] ğŸ¯ Exit decision:`, exitDecision);

    // æå– AI æ¶ˆæ¯ï¼šä¼˜å…ˆä½¿ç”¨ content å­—æ®µï¼ˆæ–°æ ¼å¼ï¼‰ï¼Œå…¼å®¹æ—§æ ¼å¼
    const aiRole = this.getConfig('ai_role', 'å’¨è¯¢å¸ˆ');
    const aiMessage = llmOutput.content || 
                      (llmOutput.response && llmOutput.response[aiRole]) || 
                      '';

    // æå–å®‰å…¨é£é™©ä¿¡æ¯
    const safetyRisk = llmOutput.safety_risk || {
      detected: false,
      risk_type: null,
      confidence: 'high',
      reason: null,
    };

    // æå–å…ƒæ•°æ®
    const llmMetadata = llmOutput.metadata || {};

    // 8. è¿”å›ç»“æœï¼ˆåŒ…å« debugInfo å’Œæ¨¡æ¿è§£æä¿¡æ¯ï¼‰
    // å¦‚æœè¾¾åˆ°æœ€å¤§è½®æ¬¡ï¼Œå¼ºåˆ¶æ ‡è®°ä¸ºå·²å®Œæˆ
    const isLastRound = this.currentRound >= this.maxRounds;
    if (isLastRound) {
      console.log(`[AiSayAction] ğŸ Reached max_rounds (${this.maxRounds}), finishing action`);
    }
    
    // ä¿®æ­£ï¼šai_say åœ¨ç¬¬ä¸€æ¬¡è¾“å‡ºæ—¶åº”è¯¥ç­‰å¾…ç”¨æˆ·ç¡®è®¤ï¼Œè€Œä¸æ˜¯ç›´æ¥å®Œæˆ
    const shouldWaitForAcknowledgment = aiMessage && this.currentRound === 1;
    
    return {
      success: true,
      completed: shouldWaitForAcknowledgment ? false : (exitDecision.should_exit || isLastRound),
      aiMessage,
      debugInfo: llmResult.debugInfo, // âœ… æ·»åŠ  debugInfo
      metadata: {
        actionType: AiSayAction.actionType,
        currentRound: this.currentRound,
        maxRounds: this.maxRounds,
        waitingFor: shouldWaitForAcknowledgment ? 'acknowledgment' : undefined,
        assessment: llmOutput.assessment || llmMetadata.assessment,
        template_path: resolution.path,
        template_layer: resolution.layer,
        template_scheme: resolution.scheme,
        safety_check: safetyCheck,
        safety_risk: safetyRisk,
        llm_metadata: llmMetadata,
        exitDecision: isLastRound
          ? {
              should_exit: true,
              reason: `è¾¾åˆ°æœ€å¤§è½®æ¬¡é™åˆ¶ (${this.maxRounds})`,
              decision_source: 'max_rounds',
            }
          : exitDecision,
      },
    };
  }

  /**
   * å…¼å®¹æ¨¡å¼æ‰§è¡Œï¼ˆä¿ç•™åŸæœ‰çš„ require_acknowledgment é€»è¾‘ï¼‰
   */
  private async executeLegacyMode(
    context: ActionContext,
    _userInput?: string | null
  ): Promise<ActionResult> {
    // 1. é€‰æ‹©åŸå§‹æ¨¡æ¿ï¼ˆä¼˜å…ˆçº§ï¼šcontent > content_template > prompt_templateï¼‰
    const rawContent = this.getConfig('content') || this.getConfig('content_template') || '';

    // æ˜ç¡®æ£€æŸ¥ require_acknowledgment
    const requireAcknowledgment = this.getConfig('require_acknowledgment', true);

    // éœ€è¦ç¡®è®¤çš„æƒ…å†µ - æ£€æŸ¥æ˜¯å¦æ˜¯ç¬¬äºŒè½®
    if (requireAcknowledgment && this.currentRound > 0) {
      console.log(`[AiSayAction] âœ… User acknowledged, action completed`);
      this.currentRound = 0;
      return {
        success: true,
        completed: true,
        aiMessage: null,
        metadata: {
          actionType: AiSayAction.actionType,
          userAcknowledged: true,
        },
      };
    }

    // 2. å˜é‡æ›¿æ¢ (ä½¿ç”¨ç»Ÿä¸€çš„æ¨¡æ¿ç®¡ç†å™¨è¿›è¡Œä¸¤å±‚æ›¿æ¢)
    const scriptVariables = this.extractScriptVariables(context);
    const systemVariables = this.buildSystemVariables(context);
    let content = this.templateManager.substituteVariables(
      rawContent,
      scriptVariables,
      systemVariables
    );

    // 3. ai_say é»˜è®¤ä½¿ç”¨ LLM ç”Ÿæˆæ›´è‡ªç„¶çš„è¡¨è¾¾
    let debugInfo;

    if (this.llmOrchestrator) {
      console.log(`[AiSayAction] ğŸ¤– Using LLM to generate natural expression`);

      const systemPrompt = `ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢å¸ˆï¼Œè¯·å°†ä»¥ä¸‹å†…å®¹æ”¹å†™ä¸ºæ›´è‡ªç„¶ã€æ›´æ¸©æš–çš„è¡¨è¾¾æ–¹å¼ï¼Œä¿æŒåŸæ„ä¸å˜ã€‚`;
      const userPrompt = `è¯·æ”¹å†™ï¼š${content}`;

      try {
        const result = await this.llmOrchestrator.generateText(`${systemPrompt}\n\n${userPrompt}`, {
          temperature: 0.7,
          maxTokens: 500,
        });

        content = result.text;
        debugInfo = result.debugInfo;
        console.log(`[AiSayAction] âœ… LLM generated: ${content.substring(0, 50)}...`);
      } catch (error: any) {
        console.error(`[AiSayAction] âŒ LLM generation failed:`, error);
      }
    } else {
      console.warn(
        `[AiSayAction] âš ï¸ LLMOrchestrator not available, using template content directly`
      );
    }

    // éœ€è¦ç¡®è®¤çš„æƒ…å†µ
    if (requireAcknowledgment) {
      this.currentRound += 1;
      return {
        success: true,
        completed: false,
        aiMessage: content,
        debugInfo,
        metadata: {
          actionType: AiSayAction.actionType,
          requireAcknowledgment: true,
          waitingFor: 'acknowledgment',
        },
      };
    }

    // ä¸éœ€è¦ç¡®è®¤
    return {
      success: true,
      completed: true,
      aiMessage: content,
      debugInfo,
      metadata: {
        actionType: AiSayAction.actionType,
        requireAcknowledgment: false,
      },
    };
  }

  /**
   * åŠ è½½æç¤ºè¯æ¨¡æ¿ï¼ˆä¸¤å±‚æ–¹æ¡ˆæœºåˆ¶ï¼‰
   */
  private async loadPromptTemplate(context: ActionContext) {
    // 1. ä» session é…ç½®è¯»å– template_scheme
    const sessionConfig = {
      template_scheme: context.metadata?.sessionConfig?.template_scheme,
    };
    
    // 2. åˆå§‹åŒ– TemplateResolverï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
    if (!this.templateResolver) {
      const projectRoot = this.resolveProjectRoot(context);
      this.templateResolver = new TemplateResolver(projectRoot);
    }
    
    // 3. è§£ææ¨¡æ¿è·¯å¾„ï¼ˆä½¿ç”¨ä¸¤å±‚è§£æï¼‰
    const resolution = await this.templateResolver.resolveTemplatePath(
      'ai_say', // æ³¨æ„ï¼šæ¨¡æ¿æ–‡ä»¶åä¸º ai_say_v1.md
      sessionConfig
    );
    
    console.log(`[AiSayAction] ğŸ“ Template resolved:`, {
      path: resolution.path,
      layer: resolution.layer,
      scheme: resolution.scheme,
      exists: resolution.exists,
    });
    
    const template = await this.templateManager.loadTemplate(resolution.path);
    
    return {
      template,
      resolution,
    };
  }

  /**
   * æå–è„šæœ¬å±‚å˜é‡
   */
  private extractScriptVariables(context: ActionContext): Map<string, any> {
    const variables = this.extractCommonProfileVariables(context);

    // æ·»åŠ æ ¸å¿ƒå†…å®¹ï¼ˆæ”¯æŒå¤šä¸ªå­—æ®µåï¼‰
    const rawContent = this.getConfig('content') || this.getConfig('content_template') || '';
    const contentWithVars = this.substituteVariables(rawContent, context);
    variables.set('topic_content', contentWithVars);

    return variables;
  }

  /**
   * æ„å»ºç³»ç»Ÿå±‚å˜é‡
   */
  private buildSystemVariables(context: ActionContext): Record<string, any> {
    return {
      time: new Date().toISOString(),
      who: context.variables['å’¨è¯¢å¸ˆå'] || 'AIå’¨è¯¢å¸ˆ',
      user: context.variables['ç”¨æˆ·å'] || 'æ¥è®¿è€…',
      chat_history: this.formatChatHistory(context.conversationHistory),
      tone: this.getConfig('tone', 'ä¸“ä¸šã€æ¸©æš–ã€å¹³å’Œ'),
      topic_content: this.extractTopicContent(context),
      understanding_threshold: this.exitCriteria?.understanding_threshold ?? 80,
      current_round: this.currentRound,
      max_rounds: this.maxRounds,
    };
  }

  /**
   * æå–è¯é¢˜å†…å®¹
   */
  private extractTopicContent(context: ActionContext): string {
    const rawContent = this.getConfig('content') || this.getConfig('content_template') || '';
    return this.substituteVariables(rawContent, context);
  }

  /**
   * æ ¼å¼åŒ–å¯¹è¯å†å²
   */
  private formatChatHistory(history: any[]): string {
    if (!history || history.length === 0) {
      return 'ï¼ˆæš‚æ— å¯¹è¯å†å²ï¼‰';
    }

    // è·å–æœ€è¿‘ 10 æ¡æ¶ˆæ¯
    const recent = history.slice(-10);
    return recent.map((msg) => `${msg.role === 'user' ? 'ç”¨æˆ·' : 'AI'}: ${msg.content}`).join('\n');
  }
}
