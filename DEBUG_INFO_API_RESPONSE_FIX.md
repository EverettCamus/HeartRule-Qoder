# LLM è°ƒè¯•ä¿¡æ¯ API å“åº”ç¼ºå¤±ä¿®å¤

## é—®é¢˜æè¿°

ç”¨æˆ·åé¦ˆåœ¨è°ƒè¯•é¢æ¿ä¸­çœ‹ä¸åˆ° LLM æç¤ºè¯å’Œ LLM å“åº”çš„è“è‰²ã€ç´«è‰²è°ƒè¯•æ°”æ³¡ï¼Œåªèƒ½çœ‹åˆ°å˜é‡çŠ¶æ€æ°”æ³¡ã€‚

## é—®é¢˜å®šä½

é€šè¿‡å‰ç«¯æ—¥å¿—åˆ†æï¼Œå‘ç° API å“åº”ä¸­ `hasDebugInfo: false`ï¼š

```javascript
[DebugChat] âœ… API Response received: {
  aiMessage: 'å‘æ¥è®¿è€…è¯¢é—®å¦‚ä½•ç§°å‘¼', 
  sessionStatus: 'active', 
  executionStatus: 'waiting_input', 
  hasVariables: true, 
  hasDebugInfo: false,  // âŒ é—®é¢˜æ‰€åœ¨
  â€¦
}
```

## æ ¹æœ¬åŸå› 

åœ¨ `packages/api-server/src/routes/sessions.ts` çš„ **POST /api/sessions/:id/messages** è·¯ç”±å¤„ç†ä¸­ï¼š

1. âœ… **SessionManager æ­£ç¡®è¿”å›äº† debugInfo**ï¼ˆç¬¬428è¡Œï¼‰ï¼š
   ```typescript
   const result = {
     aiMessage: executionState.lastAiMessage || '',
     sessionStatus: session.status,
     executionStatus: executionState.status,
     variables: executionState.variables,
     debugInfo: executionState.lastLLMDebugInfo, // âœ… è¿™é‡Œæœ‰
     position: { ... },
   };
   ```

2. âŒ **è·¯ç”±å“åº”å¯¹è±¡ä¸­ç¼ºå°‘ debugInfo å­—æ®µ**ï¼ˆç¬¬441-447è¡Œï¼‰ï¼š
   ```typescript
   const response: any = {
     aiMessage: result.aiMessage,
     sessionStatus: result.sessionStatus,
     executionStatus: result.executionStatus,
     variables: result.variables,
     position: result.position,
     // âŒ ç¼ºå°‘ debugInfo: result.debugInfo
   };
   ```

## ä¿®å¤æ–¹æ¡ˆ

åœ¨å“åº”å¯¹è±¡ä¸­æ·»åŠ  `debugInfo` å­—æ®µï¼š

```typescript
const response: any = {
  aiMessage: result.aiMessage,
  sessionStatus: result.sessionStatus,
  executionStatus: result.executionStatus,
  variables: result.variables,
  position: result.position,
  debugInfo: result.debugInfo, // âœ… æ·»åŠ  LLM è°ƒè¯•ä¿¡æ¯
};
```

## ä¿®å¤æ–‡ä»¶

**æ–‡ä»¶**ï¼š`packages/api-server/src/routes/sessions.ts`  
**è¡Œæ•°**ï¼šç¬¬447è¡Œï¼ˆåœ¨ `position: result.position,` ä¹‹åï¼‰  
**ä¿®æ”¹ç±»å‹**ï¼šæ·»åŠ ä¸€è¡Œä»£ç 

## éªŒè¯æ–¹æ³•

### 1. åç«¯éªŒè¯

ç¼–è¯‘åç«¯ä»£ç ï¼ˆå·²è‡ªåŠ¨å®Œæˆï¼‰ï¼š
```bash
cd c:\CBT\HeartRule-Qcoder
pnpm --filter api-server build
```

æœåŠ¡å™¨åº”è‡ªåŠ¨é‡å¯ï¼ˆä½¿ç”¨ tsx watch æ¨¡å¼ï¼‰ã€‚

### 2. å‰ç«¯éªŒè¯

åˆ·æ–°ç¼–è¾‘å™¨é¡µé¢ï¼Œé‡æ–°å¼€å§‹è°ƒè¯•ï¼š

1. **åˆ›å»ºæ–°è°ƒè¯•ä¼šè¯**
2. **å‘é€æ¶ˆæ¯**ï¼ˆå¦‚"ä½ å¥½"ï¼‰
3. **æ£€æŸ¥å‰ç«¯æ§åˆ¶å°æ—¥å¿—**ï¼Œåº”è¯¥çœ‹åˆ°ï¼š
   ```javascript
   [DebugChat] âœ… API Response received: {
     aiMessage: '...',
     hasDebugInfo: true,  // âœ… ç°åœ¨åº”è¯¥æ˜¯ true
     debugInfo: {
       prompt: '...',
       response: {...},
       model: '...',
       tokensUsed: ...
     }
   }
   [DebugChat] ğŸ“ Received LLM debugInfo: ...
   [DebugChat] âœ… Created LLM prompt and response bubbles
   ```

4. **æ£€æŸ¥è°ƒè¯•é¢æ¿**ï¼Œåº”è¯¥çœ‹åˆ°ï¼š
   - ğŸ”µ **è“è‰² LLM æç¤ºè¯æ°”æ³¡**ï¼ˆæ˜¾ç¤ºå‘é€ç»™ AI çš„å®Œæ•´æç¤ºï¼‰
   - ğŸŸ£ **ç´«è‰² LLM å“åº”æ°”æ³¡**ï¼ˆæ˜¾ç¤º AI çš„åŸå§‹å“åº”å†…å®¹ï¼‰
   - ğŸŸ¡ **é»„è‰²å˜é‡çŠ¶æ€æ°”æ³¡**ï¼ˆåŸæœ‰çš„å˜é‡æå–ä¿¡æ¯ï¼‰

### 3. åç«¯æ—¥å¿—éªŒè¯

åç«¯æ—¥å¿—åº”è¯¥æ˜¾ç¤ºï¼š

```
[AiSayAction] ğŸ¤– Using LLM to generate natural expression
[AiSayAction] âœ… LLM generated: ...
[ScriptExecutor] ğŸ’¾ Saved LLM debug info: {
  hasPrompt: true,
  hasResponse: true,
  model: 'deepseek-v3'
}
[SessionManager] ğŸ processUserInput completed: {
  debugInfo: { ... }  // âœ… åº”è¯¥æœ‰å†…å®¹
}
```

## ç›¸å…³é—®é¢˜å†å²

### ä¹‹å‰å·²ä¿®å¤çš„é—®é¢˜

1. âœ… **ai_say ä¸è°ƒç”¨ LLM**ï¼ˆå·²ä¿®å¤ï¼‰
   - é›†æˆ LLMOrchestrator åˆ° ScriptExecutor
   - ä¿®æ”¹ createAction æ–¹æ³•ä¼ é€’ LLMOrchestrator

2. âœ… **debugInfo æœªä¿å­˜åˆ° executionState**ï¼ˆå·²ä¿®å¤ï¼‰
   - åœ¨ executeTopic æ–¹æ³•ä¸­æ·»åŠ  debugInfo ä¿å­˜é€»è¾‘

3. âœ… **Action çŠ¶æ€æ¢å¤æ—¶ LLMOrchestrator ä¸¢å¤±**ï¼ˆå·²ä¿®å¤ï¼‰
   - ä¿®æ”¹ deserializeActionState ä½¿ç”¨ this.createAction

4. âœ… **ç§»é™¤ use_llm é…ç½®è¦æ±‚**ï¼ˆå·²ä¿®å¤ï¼‰
   - ai_say é»˜è®¤è°ƒç”¨ LLMï¼Œæ— éœ€é¢å¤–é…ç½®

### æœ¬æ¬¡ä¿®å¤çš„é—®é¢˜

5. âœ… **API å“åº”ä¸­ç¼ºå°‘ debugInfo å­—æ®µ**ï¼ˆæœ¬æ¬¡ä¿®å¤ï¼‰
   - åœ¨è·¯ç”±å“åº”å¯¹è±¡ä¸­æ·»åŠ  debugInfo å­—æ®µ

## æŠ€æœ¯ç»†èŠ‚

### æ•°æ®æµå®Œæ•´è·¯å¾„

```
ai_say Action æ‰§è¡Œ
  â†“
è°ƒç”¨ LLMOrchestrator.generateText()
  â†“
è¿”å› ActionResult { debugInfo: {...} }
  â†“
ScriptExecutor ä¿å­˜åˆ° executionState.lastLLMDebugInfo
  â†“
SessionManager è¿”å›ç»“æœåŒ…å« debugInfo
  â†“
sessions.ts è·¯ç”±åŒ…å« debugInfo åœ¨å“åº”ä¸­ â† æœ¬æ¬¡ä¿®å¤ç‚¹
  â†“
å‰ç«¯æ¥æ”¶å“åº” { debugInfo: {...} }
  â†“
DebugChatPanel åˆ›å»º LLM è°ƒè¯•æ°”æ³¡
  â†“
ç”¨æˆ·çœ‹åˆ°è“è‰²å’Œç´«è‰²æ°”æ³¡
```

### debugInfo æ•°æ®ç»“æ„

```typescript
interface LLMDebugInfo {
  prompt: string;           // ç”¨æˆ·æç¤ºè¯
  systemPrompt?: string;    // ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
  conversationHistory?: Array<{
    role: string;
    content: string;
  }>;                       // å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
  response: {
    text: string;          // LLM å“åº”æ–‡æœ¬
    // å…¶ä»–å“åº”å­—æ®µ
  };
  model: string;           // ä½¿ç”¨çš„æ¨¡å‹åç§°
  tokensUsed?: number;     // ä½¿ç”¨çš„ token æ•°é‡
  timestamp: string;       // è°ƒç”¨æ—¶é—´æˆ³
  config?: {
    temperature?: number;
    maxTokens?: number;
  };
}
```

## ä¿®å¤æ—¶é—´

**ä¿®å¤æ—¥æœŸ**ï¼š2026-01-18  
**ä¿®å¤äººå‘˜**ï¼šQoder  
**æ–‡ä»¶æ•°é‡**ï¼š1 ä¸ªæ–‡ä»¶  
**ä»£ç è¡Œæ•°**ï¼š+1 è¡Œ

## çŠ¶æ€

âœ… **å·²å®Œæˆä¿®å¤**  
â³ **ç­‰å¾…ç”¨æˆ·æµ‹è¯•éªŒè¯**

## ä¸‹ä¸€æ­¥

è¯·ç”¨æˆ·ï¼š
1. åˆ·æ–°ç¼–è¾‘å™¨é¡µé¢
2. åˆ›å»ºæ–°çš„è°ƒè¯•ä¼šè¯
3. å‘é€æ¶ˆæ¯
4. æ£€æŸ¥æ˜¯å¦èƒ½çœ‹åˆ°è“è‰²å’Œç´«è‰²çš„ LLM è°ƒè¯•æ°”æ³¡
5. å¦‚æœ‰é—®é¢˜ï¼Œæä¾›æ–°çš„å‰ç«¯æ§åˆ¶å°æ—¥å¿—å’Œåç«¯æ—¥å¿—
